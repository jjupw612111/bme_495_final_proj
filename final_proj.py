# -*- coding: utf-8 -*-
"""final proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kSc9-JSNSL3zfD_aY25oIuBeIhunoleP
"""

# --- Step 2: Define the PyTorch Dataset for 3D Volumetric Data ---
import glob
import nibabel as nib
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import sys, os
data_dir = '/content/drive/MyDrive/cirrmri/CirrMRI600+.zip'

!unzip '/content/drive/MyDrive/cirrmri/CirrMRI600+.zip' -d /content/drive/MyDrive/cirrmri/CirrMRI600+

!unzip '/content/drive/MyDrive/cirrmri/CirrMRI600+/Cirrhosis_T2_3D.zip' -d /content/drive/MyDrive/cirrmri/CirrMRI600+/Cirrhosis_T2_3D

"""

```
/content/drive/MyDrive/cirrmri/CirrMRI600+/
    Cirrhosis_T1_3D/
        Cirrhosis_T1_3D/
            train_images/
            train_masks/
            valid_images/
            valid_masks/
            test_images/
            test_masks/
    Cirrhosis_T2_3D/
        Cirrhosis_T2_3D/
            train_images/
            train_masks/
            valid_images/
            valid_masks/
            test_images/
            test_masks/
```

"""

import nibabel as nib
import pickle as pkl
#segmentation map --> mask
def read_nii_files(img_file_path):
  """
  example filepath:  extracting: /content/drive/MyDrive/cirrmri/CirrMRI600+/Cirrhosis_T1_3D/Cirrhosis_T1_3D/train_masks/346.nii.gz
  path:

  """
  # Construct the file path
  # Load and return the image data
  img = nib.load(img_file_path).get_fdata().astype(np.float32)
  return img

import nibabel as nib
import matplotlib.pyplot as plt
import numpy as np

# File paths for the image and mask.
# (Adjust these paths as needed.)
image_path = '/content/drive/MyDrive/cirrmri/CirrMRI600+/Cirrhosis_T2_3D/Cirrhosis_T2_3D/valid_images/50.nii.gz'
mask_path  = '/content/drive/MyDrive/cirrmri/CirrMRI600+/Cirrhosis_T2_3D/Cirrhosis_T2_3D/valid_masks/50.nii.gz'

# Load the image and mask volumes using nibabel

img_data = read_nii_files(image_path)
mask_data = read_nii_files(mask_path)
print(f"Image shape: {img_data.shape}")
print(f"Mask shape: {mask_data.shape}")

# Select a slice index.
# Here, we choose the middle slice along the third axis.
slice_index = img_data.shape[2] // 2

# Extract the corresponding 2D slice from both image and mask
img_slice = img_data[:, :, slice_index]
mask_slice = mask_data[:, :, slice_index]

# Plot the image and mask in separate subplots.
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Display the image slice (in grayscale)
axes[0].imshow(img_slice, cmap='gray')
axes[0].set_title('Image Slice (ID = 50)')
axes[0].axis('off')

# Display the mask slice (in grayscale; change cmap if needed)
axes[1].imshow(mask_slice, cmap='gray')
axes[1].set_title('Mask Slice (ID = 50)')
axes[1].axis('off')

plt.show()

# Plot an overlay where the mask is translucent over the image.
plt.figure(figsize=(6, 6))
plt.imshow(img_slice, cmap='gray')
# Overlay the mask using a different colormap (e.g., 'jet') with some transparency
plt.imshow(mask_slice, cmap='jet', alpha=0.5)
plt.title('Overlay: Image with Predicted Mask (ID = 50)')
plt.axis('off')
plt.show()

!pip install monai

!git clone https://github.com/jjupw612111/bme_495_final_proj.git

!ls /content

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary
import time

#vanilla unet for 3d image segmentation https://github.com/aghdamamir/3D-UNet/blob/main/unet3d.py
# exampel of 3d mri medium article: https://medium.com/@rehman.aimal/implement-3d-unet-for-cardiac-volumetric-mri-scans-in-pytorch-79f8cca7dc68  https://github.com/aimalrehman92/CardiacMRI_3D_UNet_Pytorch/blob/master/3D_Cardiac_UNet.ipynb
#3d unet paper https://arxiv.org/pdf/1606.06650v1
#dual stream implementation attempt with chatgpt and diagram from paper:
#   Elghazy et al. #https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12708
# Ronneberger et al. 2015 https://arxiv.org/pdf/1505.04597#page=1.96

"""
Dual-Stream 3D UNet Architecture based on:
"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"
Adapted to process two modalities (T1 and T2) with separate decoders.
Each encoder branch is identical, and their corresponding outputs are fused
using 1x1x1 convolution. Then, two separate decoders (synthesis paths) produce
modality-specific segmentation outputs.
"""
class Conv3DBlock(nn.Module):
    """
    Basic block for double 3x3x3 convolutions with InstanceNorm3d and ReLU.
    If bottleneck==False, a 2x2x2 max pooling is applied.
    """
    def __init__(self, in_channels, out_channels, bottleneck=False):
        super(Conv3DBlock, self).__init__()
        # Reduce memory: Use fewer channels.
        self.conv1 = nn.Conv3d(in_channels, out_channels // 2, kernel_size=3, padding=1)
        self.norm1 = nn.InstanceNorm3d(out_channels // 2)  # ✅ InstanceNorm3d
        self.conv2 = nn.Conv3d(out_channels // 2, out_channels, kernel_size=3, padding=1)
        self.norm2 = nn.InstanceNorm3d(out_channels)  # ✅ InstanceNorm3d
        self.relu = nn.ReLU(inplace=True)
        self.bottleneck = bottleneck
        if not self.bottleneck:
            self.pooling = nn.MaxPool3d(kernel_size=2, stride=2)

    def forward(self, x):
        out = self.relu(self.norm1(self.conv1(x)))
        out = self.relu(self.norm2(self.conv2(out)))
        if self.bottleneck:
            return out, out  # Return same feature for skip (won't be used) and for further processing.
        else:
            return self.pooling(out), out  # Return pooled output and residual (skip connection)

class UpConv3DBlock(nn.Module):
    """
    Decoder block: up-convolves with a 2x2x2 transpose convolution, concatenates with the corresponding
    encoder residual (skip) connection, and applies two 3x3x3 convolutions with InstanceNorm3d and ReLU.
    """
    def __init__(self, in_channels, res_channels=0, last_layer=False, num_classes=None):
        super(UpConv3DBlock, self).__init__()
        self.upconv1 = nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2)
        self.conv1 = nn.Conv3d(in_channels + res_channels, in_channels // 2, kernel_size=3, padding=1)
        self.norm1 = nn.InstanceNorm3d(in_channels // 2)  # ✅ InstanceNorm3d
        self.conv2 = nn.Conv3d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1)
        self.norm2 = nn.InstanceNorm3d(in_channels // 2)  # ✅ InstanceNorm3d
        self.relu = nn.ReLU(inplace=True)
        self.last_layer = last_layer
        if self.last_layer:
            assert num_classes is not None, "num_classes must be provided for the last layer"
            self.conv3 = nn.Conv3d(in_channels // 2, num_classes, kernel_size=1)

    def forward(self, x, residual):
        x = self.upconv1(x)
        if x.shape[2:] != residual.shape[2:]:
            x = F.interpolate(x, size=residual.shape[2:], mode='trilinear', align_corners=True)  # Force shape match

            residual = F.interpolate(residual, size=x.shape[2:], mode='trilinear', align_corners=True)
        x = torch.cat([x, residual], dim=1)
        x = self.relu(self.norm1(self.conv1(x)))
        x = self.relu(self.norm2(self.conv2(x)))
        if self.last_layer:
            x = self.conv3(x)
        return x

########################################
# Single-Stream UNet3D (from your provided code)
########################################

class UNet3D(nn.Module):
    def __init__(self, in_channels, num_classes, level_channels=[16, 32, 64], bottleneck_channel=128):
        super(UNet3D, self).__init__()
        level1, level2, level3 = level_channels
        self.a_block1 = Conv3DBlock(in_channels, level1)
        self.a_block2 = Conv3DBlock(level1, level2)
        self.a_block3 = Conv3DBlock(level2, level3)
        self.bottleNeck = Conv3DBlock(level3, bottleneck_channel, bottleneck=True)
        self.s_block3 = UpConv3DBlock(bottleneck_channel, res_channels=level3)
        self.s_block2 = UpConv3DBlock(level3, res_channels=level2)
        self.s_block1 = UpConv3DBlock(level2, res_channels=level1, last_layer=True, num_classes=num_classes)

    def forward(self, x):
        # Analysis path
        out, res1 = self.a_block1(x)
        out, res2 = self.a_block2(out)
        out, res3 = self.a_block3(out)
        out, _ = self.bottleNeck(out)  # Bottleneck
        # Synthesis path
        out = self.s_block3(out, res3)
        out = self.s_block2(out, res2)
        out = self.s_block1(out, res1)
        return out

########################################
# Dual-Stream UNet3D with Separate Decoders for T1 and T2
########################################

class DualStreamUNet3D(nn.Module):
    def __init__(self, in_channels=3, num_classes=1, level_channels=[64, 128, 256], bottleneck_channel=512):
        """
        Dual-stream architecture that builds two UNet3D branches for T1 and T2.
        At each corresponding level (skip connections and bottleneck), features from both streams
        are fused via concatenation followed by a 1×1 convolution. Then, each branch's decoder is run separately.
        """
        super(DualStreamUNet3D, self).__init__()
        level1, level2, level3 = level_channels

        # Analysis paths for T1 and T2
        self.a_block1_T1 = Conv3DBlock(in_channels, level1)
        self.a_block2_T1 = Conv3DBlock(level1, level2)
        self.a_block3_T1 = Conv3DBlock(level2, level3)
        self.bottleNeck_T1 = Conv3DBlock(level3, bottleneck_channel, bottleneck=True)

        self.a_block1_T2 = Conv3DBlock(in_channels, level1)
        self.a_block2_T2 = Conv3DBlock(level1, level2)
        self.a_block3_T2 = Conv3DBlock(level2, level3)
        self.bottleNeck_T2 = Conv3DBlock(level3, bottleneck_channel, bottleneck=True)

        # Fusion layers for skip connections and bottleneck.
        self.fuse_skip1 = nn.Conv3d(level1 * 2, level1, kernel_size=1)
        self.fuse_skip2 = nn.Conv3d(level2 * 2, level2, kernel_size=1)
        self.fuse_skip3 = nn.Conv3d(level3 * 2, level3, kernel_size=1)
        self.fuse_bottle = nn.Conv3d(bottleneck_channel * 2, bottleneck_channel, kernel_size=1)

        # Synthesis (decoder) paths for T1 and T2 (separate decoders)
        # For T1
        self.s_block3_T1 = UpConv3DBlock(bottleneck_channel, res_channels=level3)
        self.s_block2_T1 = UpConv3DBlock(level3, res_channels=level2)
        self.s_block1_T1 = UpConv3DBlock(level2, res_channels=level1, last_layer=True, num_classes=num_classes)
        # For T2
        self.s_block3_T2 = UpConv3DBlock(bottleneck_channel, res_channels=level3)
        self.s_block2_T2 = UpConv3DBlock(level3, res_channels=level2)
        self.s_block1_T2 = UpConv3DBlock(level2, res_channels=level1, last_layer=True, num_classes=num_classes)

    def forward(self, x):
        """
        Expects input x of shape [B, 2, C, D, H, W] where:
         - x[:,0] is the T1 modality,
         - x[:,1] is the T2 modality.
        Returns:
         - pred_T1: segmentation prediction for T1.
         - pred_T2: segmentation prediction for T2.
        """
        # Split input into T1 and T2 (each of shape [B, C, D, H, W])
        x_T1 = x[:, 0, ...]
        x_T2 = x[:, 1, ...]

        # Analysis path for T1
        out1, res1_T1 = self.a_block1_T1(x_T1)
        out1, res2_T1 = self.a_block2_T1(out1)
        out1, res3_T1 = self.a_block3_T1(out1)
        out1, bottle_T1 = self.bottleNeck_T1(out1)

        # Analysis path for T2
        out2, res1_T2 = self.a_block1_T2(x_T2)
        out2, res2_T2 = self.a_block2_T2(out2)
        out2, res3_T2 = self.a_block3_T2(out2)
        out2, bottle_T2 = self.bottleNeck_T2(out2)

        # Fuse skip connections and bottleneck outputs:
        fuse_res1 = self.fuse_skip1(torch.cat([res1_T1, res1_T2], dim=1))
        fuse_res2 = self.fuse_skip2(torch.cat([res2_T1, res2_T2], dim=1))
        fuse_res3 = self.fuse_skip3(torch.cat([res3_T1, res3_T2], dim=1))
        fuse_bottle = self.fuse_bottle(torch.cat([bottle_T1, bottle_T2], dim=1))

        # Synthesis path for T1 (using the fused features)
        d3_T1 = self.s_block3_T1(fuse_bottle, fuse_res3)
        d2_T1 = self.s_block2_T1(d3_T1, fuse_res2)
        pred_T1 = self.s_block1_T1(d2_T1, fuse_res1)

        # Synthesis path for T2 (using the same fused features but different decoder weights)
        d3_T2 = self.s_block3_T2(fuse_bottle, fuse_res3)
        d2_T2 = self.s_block2_T2(d3_T2, fuse_res2)
        pred_T2 = self.s_block1_T2(d2_T2, fuse_res1)

        return pred_T1, pred_T2

########################################
# Testing the Dual-Stream UNet3D
########################################

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/bme_495_final_proj/bme_495_final_proj/bme_495_final_proj
import sys
# Add the current directory to the Python path
sys.path.append(os.getcwd())

from unet3d import UNet3D, DualStreamUNet3D
from dataset import CirrMRI3DDataset, get_dataloaders

import os
import nibabel as nib
import torch
from torch.utils.data import Dataset, DataLoader
from monai.transforms import (
    Compose,
    ToTensord,
    RandFlipd,
    Spacingd,
    RandScaleIntensityd,
    RandShiftIntensityd,
    NormalizeIntensityd,
    EnsureChannelFirstd,
    DivisiblePadd,
    ResizeWithPadOrCropd  # Added for resizing
)

def extract_id(filename):
    base = os.path.basename(filename)
    if base.endswith('.nii.gz'):
        base = base[:-7]  # Remove '.nii.gz'
    elif base.endswith('.nii'):
        base = base[:-4]  # Remove '.nii'
    return int(base)



class CirrMRI3DDataset(Dataset):
    def __init__(self, base_dir, modality, split='train', transform=None):
        """
        Args:
            base_dir (str): Base directory path, e.g. "/content/drive/MyDrive/cirrmri/CirrMRI600+"
            modality (str): Either "Cirrhosis_T1_3D" or "Cirrhosis_T2_3D"
            split (str): One of 'train', 'valid', or 'test'
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.transform = transform

        # Construct the directories for images and masks.
        # Example: .../Cirrhosis_T1_3D/Cirrhosis_T1_3D/train_images
        self.image_dir = os.path.join(base_dir, modality, modality, f"{split}_images")
        self.mask_dir = os.path.join(base_dir, modality, modality, f"{split}_masks")

        # List all nii.gz files in the images directory (assuming masks have the same names)
        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith('.nii.gz')])
        if not self.image_files:
            raise RuntimeError(f"No .nii.gz files found in {self.image_dir}")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Get the file name based on the index
        img_filename = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_filename)
        mask_path = os.path.join(self.mask_dir, img_filename)  # Assuming same filename for mask

        # Load the image and mask using nibabel
        img_nii = nib.load(img_path)
        mask_nii = nib.load(mask_path)
        img_array = img_nii.get_fdata()
        mask_array = mask_nii.get_fdata()

        # Compute true volume from the original mask (before any transforms)
        # Here, we assume the mask is binary (segmented region > 0)
        voxel_dims = mask_nii.header.get_zooms()  # voxel dimensions (e.g. in mm)
        voxel_volume = float(voxel_dims[0] * voxel_dims[1] * voxel_dims[2])
        true_volume = (mask_array > 0).sum() * voxel_volume

        # Optionally: Convert the numpy arrays to PyTorch tensors.
        img_tensor = torch.tensor(img_array, dtype=torch.float32)
        mask_tensor = torch.tensor(mask_array, dtype=torch.long)

        # Add a channel dimension if your network expects one.
        if img_tensor.ndim == 3:
            img_tensor = img_tensor.unsqueeze(0)
        if mask_tensor.ndim == 3:
            mask_tensor = mask_tensor.unsqueeze(0)

        # Prepare the sample dictionary.
        sample = {
            'image': img_tensor,
            'mask': mask_tensor,
            'id': extract_id(img_filename),
            'true_volume': true_volume
        }

        # Apply transformation if provided
        if self.transform:
            sample = self.transform(sample)

        return sample

# # Example transformations remain the same.
# train_transforms = Compose([
#     EnsureChannelFirstd(keys=['image', 'mask']),
#     Spacingd(keys=['image', 'mask'], pixdim=(1.0, 1.0, 1.0), mode=('bilinear', 'nearest')),
#     RandFlipd(keys=['image', 'mask'], prob=0.5, spatial_axis=0),
#     RandFlipd(keys=['image', 'mask'], prob=0.5, spatial_axis=1),
#     RandFlipd(keys=['image', 'mask'], prob=0.5, spatial_axis=2),
#     RandScaleIntensityd(keys=['image'], factors=0.1, prob=0.5),
#     RandShiftIntensityd(keys=['image'], offsets=0.1, prob=0.5),
#     NormalizeIntensityd(keys=['image'], nonzero=True, channel_wise=True),
#     DivisiblePadd(keys=['image', 'mask'], k=16),
#     ResizeWithPadOrCropd(keys=['image', 'mask'], spatial_size=(128, 128, 40)),
#     ToTensord(keys=['image', 'mask'])
# ])

# val_transforms = Compose([
#     EnsureChannelFirstd(keys=['image', 'mask']),
#     Spacingd(keys=['image', 'mask'], pixdim=(1.0, 1.0, 1.0), mode=('bilinear', 'nearest')),
#     NormalizeIntensityd(keys=['image'], nonzero=True, channel_wise=True),
#     ResizeWithPadOrCropd(keys=['image', 'mask'], spatial_size=(128, 128, 40)),
#     ToTensord(keys=['image', 'mask'])
# ])

# # Minimal transformations without augmentations.
# common_transforms = Compose([
#     EnsureChannelFirstd(keys=['image', 'mask'], channel_dim=0),
#     Spacingd(keys=['image', 'mask'], pixdim=(1.0, 1.0, 1.0), mode=('bilinear', 'nearest')),
#     NormalizeIntensityd(keys=['image'], nonzero=True, channel_wise=True),
#     ResizeWithPadOrCropd(keys=['image', 'mask'], spatial_size=(128, 128, 40)),
#     ToTensord(keys=['image', 'mask'])
# ])
from monai.transforms import Compose, EnsureChannelFirstd, Spacingd, NormalizeIntensityd, Resized, ToTensord

# Minimal transformations without augmentations.
common_transforms = Compose([
    EnsureChannelFirstd(keys=['image', 'mask'], channel_dim=0),
    Spacingd(keys=['image', 'mask'], pixdim=(1.0, 1.0, 1.0), mode=('bilinear', 'nearest')),
    NormalizeIntensityd(keys=['image'], nonzero=True, channel_wise=True),
    Resized(keys=['image', 'mask'], spatial_size=(128, 128, 40), mode=('trilinear', 'nearest')),
    ToTensord(keys=['image', 'mask'])
])

# For both training and validation, use the same minimal preprocessing.
train_transforms = common_transforms
val_transforms = common_transforms
def get_dataloaders(base_dir, modality, train_batch_size=2, valid_batch_size=1, test_batch_size=2):
    """
    Utility function to create dataloaders for train, valid, and test splits.
    """
    train_dataset = CirrMRI3DDataset(base_dir, modality, split='train', transform=train_transforms)
    valid_dataset = CirrMRI3DDataset(base_dir, modality, split='valid', transform=val_transforms)
    test_dataset  = CirrMRI3DDataset(base_dir, modality, split='test', transform=val_transforms)

    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0)
    valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False, num_workers=0)
    test_loader  = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)

    return train_loader, valid_loader, test_loader

# Example usage:
# base_dir = "/content/drive/MyDrive/cirrmri/CirrMRI600+"
# modality = "Cirrhosis_T1_3D"  # or "Cirrhosis_T2_3D"
# train_loader, valid_loader, test_loader = get_dataloaders(base_dir, modality)

# For testing purposes only:
test_transforms = Compose([
    EnsureChannelFirstd(keys=['image', 'mask'], channel_dim=0),
    NormalizeIntensityd(keys=['image'], nonzero=True, channel_wise=True),
    ResizeWithPadOrCropd(keys=['image', 'mask'], spatial_size=(128, 128, 40)),
    ToTensord(keys=['image', 'mask'])
])

# Create a small dataset using the test transforms.
test_dataset = CirrMRI3DDataset(base_dir, "Cirrhosis_T1_3D", split='test', transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

# Iterate through a few batches to see if OOM still occurs.
for batch in test_loader:
    images = batch['image']
    print(f"Image shape: {images.shape}")
    break

base_dir = "/content/drive/MyDrive/cirrmri/CirrMRI600+"
T1_path = "Cirrhosis_T1_3D"
T2_path = "Cirrhosis_T2_3D"

T1_train_loader, T1_valid_loader, T1_test_loader = get_dataloaders(base_dir, T1_path)
T2_train_loader, T2_valid_loader, T2_test_loader = get_dataloaders(base_dir, T2_path)

"""The learning rate was
initially set to 0.0001 and was gradually decreased using
the Cosine Annealing Scheduler. We use the BCE-Dice
loss with the AdamW optimizer with a batch size of 4. The
models were trained for 500 epochs with an early stopping
patience of 50 epochs to prevent overfitting. The learning
rate decay was set at 0.001 after every 10 epochs. To accelerate training, we leveraged two Nvidia A6000 GPUs, each
with 48GB of memory, and utilized PyTorch’s Distributed
Data Parallel to distribute a batch of 4 to each GPU. We
resized every volume to uniform spatial dimensions of
256 × 256 × 80 for generalizability.
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt, binary_erosion
from scipy.spatial.distance import directed_hausdorff
from scipy.stats import pearsonr
from tqdm import tqdm  # Import tqdm for progress bars
from torch.amp import autocast, GradScaler

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
#%tensorboard --logdir runs
# %tensorboard --logdir=runs/unet3d_experiment

# -------------------------
# Loss: BCE-Dice
# -------------------------
class BCEDiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(BCEDiceLoss, self).__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.smooth = smooth

    def forward(self, logits, targets):
        bce_loss = self.bce(logits, targets)
        preds = torch.sigmoid(logits)
        intersection = (preds * targets).sum(dim=[1,2,3,4])
        dice_loss = 1 - (2 * intersection + self.smooth) / (
            preds.sum(dim=[1,2,3,4]) + targets.sum(dim=[1,2,3,4]) + self.smooth)
        dice_loss = dice_loss.mean()
        return bce_loss + dice_loss

# -------------------------
# Metrics (computed on numpy arrays)
# -------------------------
def dice_metric(pred, target, smooth=1e-6):
    intersection = np.sum(pred * target)
    return (2 * intersection + smooth) / (np.sum(pred) + np.sum(target) + smooth)

def iou_metric(pred, target, smooth=1e-6):
    intersection = np.sum(pred * target)
    union = np.sum(pred) + np.sum(target) - intersection
    return (intersection + smooth) / (union + smooth)

def precision_metric(pred, target, smooth=1e-6):
    tp = np.sum(pred * target)
    predicted_positive = np.sum(pred)
    return (tp + smooth) / (predicted_positive + smooth)

def recall_metric(pred, target, smooth=1e-6):
    tp = np.sum(pred * target)
    actual_positive = np.sum(target)
    return (tp + smooth) / (actual_positive + smooth)

def hausdorff_distance(pred, target):
    pred_coords = np.array(np.nonzero(pred)).T
    target_coords = np.array(np.nonzero(target)).T
    if len(pred_coords) == 0 or len(target_coords) == 0:
        return np.nan
    d1 = directed_hausdorff(pred_coords, target_coords)[0]
    d2 = directed_hausdorff(target_coords, pred_coords)[0]
    return max(d1, d2)

def assd_metric(pred, target):
    pred_boundary = pred - binary_erosion(pred)
    target_boundary = target - binary_erosion(target)
    pred_boundary_coords = np.array(np.nonzero(pred_boundary)).T
    target_boundary_coords = np.array(np.nonzero(target_boundary)).T
    if len(pred_boundary_coords) == 0 or len(target_boundary_coords) == 0:
        return np.nan
    dt_target = distance_transform_edt(1 - target_boundary)
    dt_pred = distance_transform_edt(1 - pred_boundary)
    distances_pred = dt_target[np.nonzero(pred_boundary)]
    distances_target = dt_pred[np.nonzero(target_boundary)]
    return (np.mean(distances_pred) + np.mean(distances_target)) / 2.0

# -------------------------
# Training and Evaluation Functions
# -------------------------
def train(model, optimizer, train_loader, device, criterion, scaler):
    model.train()
    running_loss = 0.0
    # Use tqdm progress bar for batches
    for batch in tqdm(train_loader, desc="Training", leave=False):

        optimizer.zero_grad()
        images = batch['image'].to(device)   # (B, 1, H, W, D)
        masks = batch['mask'].to(device)       # (B, H, W, D)
        #masks = masks.unsqueeze(1).float()     # (B, 1, H, W, D)
        with torch.autocast(device_type="cuda" if torch.cuda.is_available() else "cpu"):

          outputs = model(images)                # (B, 1, H, W, D)
          loss = criterion(outputs, masks)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        # outputs = model(images)                # (B, 1, H, W, D)
        # loss = criterion(outputs, masks)
        # loss.backward()
        # optimizer.step()
        running_loss += loss.item() * images.size(0)
    epoch_loss = running_loss / len(train_loader.dataset)
    return epoch_loss

def evaluate(model, loader, device, criterion):
    model.eval()
    running_loss = 0.0
    dice_scores = []
    iou_scores = []
    precisions = []
    recalls = []
    hausdorffs = []
    assd_list = []
    # Use tqdm progress bar for evaluation loop
    for batch in tqdm(loader, desc="Evaluating", leave=False):
        images = batch['image'].to(device)
        masks = batch['mask'].to(device)

        #masks = masks.unsqueeze(1).float()
        outputs = model(images)
        loss = criterion(outputs, masks)
        running_loss += loss.item() * images.size(0)
        preds = torch.sigmoid(outputs)
        preds_bin = (preds > 0.5).float()
        for i in range(images.size(0)):
            pred_np = preds_bin[i, 0].cpu().numpy()
            mask_np = masks[i, 0].cpu().numpy()
            dice_scores.append(dice_metric(pred_np, mask_np))
            iou_scores.append(iou_metric(pred_np, mask_np))
            precisions.append(precision_metric(pred_np, mask_np))
            recalls.append(recall_metric(pred_np, mask_np))
            hausdorffs.append(hausdorff_distance(pred_np, mask_np))
            assd_list.append(assd_metric(pred_np, mask_np))
    epoch_loss = running_loss / len(loader.dataset)
    metrics = {
        'dice': np.nanmean(dice_scores),
        'iou': np.nanmean(iou_scores),
        'precision': np.nanmean(precisions),
        'recall': np.nanmean(recalls),
        'hausdorff': np.nanmean(hausdorffs),
        'assd': np.nanmean(assd_list)
    }
    return epoch_loss, metrics

def train_model(model, train_loader, valid_loader, device, num_epochs=500, early_stopping_patience=50):
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = BCEDiceLoss()

    best_val_loss = float('inf')
    best_epoch = 0
    best_model_wts = None

    # Initialize SummaryWriter to log losses and metrics
    writer = SummaryWriter(log_dir="runs/unet3d_experiment")
    scaler = torch.cuda.amp.GradScaler()

    for epoch in range(num_epochs):
        t0 = time.time()
        train_loss = train(model, optimizer,train_loader, device, criterion, scaler)
        val_loss, metrics = evaluate(model, valid_loader, device, criterion)
        scheduler.step()  # Update learning rate

        elapsed = time.time() - t0
        print(f"Epoch {epoch+1}/{num_epochs} | Time: {elapsed:.1f}s")
        print(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        print(f"  Metrics: Dice: {metrics['dice']:.4f}, IoU: {metrics['iou']:.4f}, "
              f"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, "
              f"Hausdorff: {metrics['hausdorff']:.4f}, ASSD: {metrics['assd']:.4f}")

        # Log losses and metrics to TensorBoard
        writer.add_scalar("Loss/Train", train_loss, epoch)
        writer.add_scalar("Loss/Val", val_loss, epoch)
        writer.add_scalar("Metrics/Dice", metrics['dice'], epoch)
        writer.add_scalar("Metrics/IoU", metrics['iou'], epoch)
        writer.add_scalar("Metrics/Precision", metrics['precision'], epoch)
        writer.add_scalar("Metrics/Recall", metrics['recall'], epoch)
        writer.add_scalar("Metrics/Hausdorff", metrics['hausdorff'], epoch)
        writer.flush() #flush logs after each epoch

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            best_model_wts = model.state_dict()
            torch.save(model.state_dict(), "model_save.pth")

        elif epoch - best_epoch >= early_stopping_patience:
            print("Early stopping triggered.")
            break
        torch.cuda.empty_cache() #clears gpu cache after epoch


    if best_model_wts is not None:
        model.load_state_dict(best_model_wts)

    writer.close()
    return model

# -------------------------
# Saving and Plotting Functions
# -------------------------
def save_model(model, path):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def plot_volume_correlation(model, test_loader, device, threshold=0.5):
    model.eval()
    gt_volumes = []
    pred_volumes = []
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Volume Correlation", leave=False):
            images = batch['image'].to(device)
            true_vols = batch['true_volume']  # Provided by the dataset.
            outputs = model(images)
            preds = torch.sigmoid(outputs)
            preds_bin = (preds > threshold).float()
            for i in range(images.size(0)):
                pred_np = preds_bin[i, 0].cpu().numpy()
                pred_vol = np.sum(pred_np)
                pred_volumes.append(pred_vol)
                gt_volumes.append(float(true_vols[i]))
    corr_coef, p_val = pearsonr(gt_volumes, pred_volumes)
    print(f"Pearson correlation: {corr_coef:.4f} (p={p_val:.4e})")

    plt.figure(figsize=(6,6))
    plt.scatter(gt_volumes, pred_volumes, alpha=0.7)
    plt.xlabel("Ground Truth Volume")
    plt.ylabel("Predicted Volume")
    plt.title("Volume Correlation")
    max_val = max(max(gt_volumes), max(pred_volumes))
    plt.plot([0, max_val], [0, max_val], 'r--')
    plt.show()

def plot_sample_overlay(model, test_loader, device, threshold=0.5, slice_idx=None):
    model.eval()
    batch = next(iter(test_loader))
    image = batch['image'][0].unsqueeze(0).to(device)  # (1, 1, H, W, D)
    mask = batch['mask'][0].numpy()                     # (H, W, D)
    with torch.no_grad():
        output = model(image)
        pred = torch.sigmoid(output)
        pred_bin = (pred > threshold).float()[0, 0].cpu().numpy()

    # H, W, D = mask.shape
    mask = mask.squeeze()  # Remove batch dimension if necessary

    if len(mask.shape) == 2:  # If it's 2D, treat depth as 1
        H, W = mask.shape
        D = 1
    elif len(mask.shape) == 3:
        H, W, D = mask.shape
    else:
        raise ValueError(f"Unexpected mask shape: {mask.shape}")

    if slice_idx is None:
        slice_idx = D // 2

    img_slice = image[0, 0, :, :, slice_idx].cpu().numpy()
    mask_slice = mask[:, :, slice_idx]
    pred_slice = pred_bin[:, :, slice_idx]

    plt.figure(figsize=(8,6))
    plt.imshow(img_slice, cmap='gray')
    #plt.imshow(np.ma.masked_where(mask_slice == 0, mask_slice), cmap='Reds', alpha=0.4)
    plt.imshow(np.ma.masked_where(pred_slice == 0, pred_slice), cmap='Blues', alpha=0.4)
    plt.title("Overlay: Image with Ground Truth (Red) and Prediction (Blue)")
    plt.axis('off')
    plt.show()

model = UNet3D(1, 1)
model.load_state_dict(torch.load("model_save.pth", weights_only=True))
model = model.to(device)

import matplotlib.pyplot as plt
import numpy as np

def plot_sample_overlay(model, test_loader, device, threshold=0.5, slice_idx=None):
    model.eval()
    batch = next(iter(test_loader))

    image = batch['image'][0].unsqueeze(0).to(device)  # Keep batch & channel dimensions
    mask = batch['mask'][0].squeeze().numpy()

    with torch.no_grad():
        output = model(image)
        pred = torch.sigmoid(output)
        pred_bin = (pred > threshold).float()[0, 0].cpu().numpy()  # Convert to binary mask

    # Ensure mask and prediction have correct dimensions
    mask = mask.squeeze()
    pred_bin = pred_bin.squeeze()

    if len(mask.shape) == 2:
        H, W = mask.shape
        D = 1
    elif len(mask.shape) == 3:
        H, W, D = mask.shape
    else:
        raise ValueError(f"Unexpected mask shape: {mask.shape}")

    if slice_idx is None:
        slice_idx = D // 2  # Middle slice
        # Extract the corresponding MRI slice
    mri_slice = image.cpu().numpy().squeeze()[:, :, slice_idx]  # Extract channel 0
# Plot MRI, Ground Truth, and Overlay
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(mri_slice, cmap='gray')  # Display MRI image
    plt.title("MRI Slice")

    plt.subplot(1, 3, 2)
    plt.imshow(mask[:, :, slice_idx], cmap='gray')  # Ground truth mask
    plt.title("Ground Truth Mask")

    plt.subplot(1, 3, 3)
    plt.imshow(mri_slice, cmap='gray')  # Background MRI
    plt.imshow(pred_bin[:, :, slice_idx], cmap='jet', alpha=0.5)  # Overlay
    plt.title("MRI + Prediction Overlay")

    plt.show()

plot_sample_overlay(model, T1_test_loader, device, threshold=0.5)

torch.cuda.empty_cache()

# -------------------------
# Main Training, Saving, and Plotting for T1 and T2
# -------------------------

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assume dataloaders are built as:
# T1_train_loader, T1_valid_loader, T1_test_loader = get_dataloaders(base_dir, T1_path)
# T2_train_loader, T2_valid_loader, T2_test_loader = get_dataloaders(base_dir, T2_path)

# ---- Train on T1 images ----
print("Training UNet3D on T1 images")
model_T1 = UNet3D(in_channels=1, num_classes=1, level_channels=[16,32,64], bottleneck_channel=128).to(device)
summary(model_T1, input_size=(1, 128, 128, 40), batch_size=1)
model_T1 = train_model(model_T1, T1_train_loader, T1_valid_loader, device,
                        num_epochs=50, early_stopping_patience=5)

save_model(model_T1, "unet3d_t1.pth")

test_loss_T1, metrics_T1 = evaluate(model_T1, T1_test_loader, device, BCEDiceLoss())
print("T1 Test Metrics:", metrics_T1)

plot_volume_correlation(model_T1, T1_test_loader, device, threshold=0.5)
plot_sample_overlay(model_T1, T1_test_loader, device, threshold=0.5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ---- Train on T2 images ----
print("\nTraining UNet3D on T2 images")
model_T2 = UNet3D(in_channels=1, num_classes=1, level_channels=[16,32,64], bottleneck_channel=128).to(device)
summary(model_T2, input_size=(1, 128,128,40), batch_size=1)
model_T2 = train_model(model_T2, T2_train_loader, T2_valid_loader, device,
                        num_epochs=50, early_stopping_patience=5)
test_loss_T2, metrics_T2 = evaluate(model_T2, T2_test_loader, device, BCEDiceLoss())
print("T2 Test Metrics:", metrics_T2)
save_model(model_T2, "unet3d_t2.pth")
plot_volume_correlation(model_T2, T2_test_loader, device, threshold=0.5)
plot_sample_overlay(model_T2, T2_test_loader, device, threshold=0.5)